{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project - Email Spam Corpera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Brandon Croarkin, Michelle Mak, and T.S. Yeap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open python and nltk packages needed for processing\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentence_polarity\n",
    "from nltk.collocations import *\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(r'C:\\Users\\brcro\\OneDrive\\Documents\\Syracuse\\IST 664 - Natural Language Processing\\Final Project\\FinalProjectData\\EmailSpamCorpora\\corpus')\n",
    "os.chdir(r'C:\\Users\\Brandon Croarkin\\Documents\\Education\\Syracuse\\NLP\\Final Project\\FinalProjectData\\EmailSpamCorpora\\corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'ham', 'spam']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features (keywords) of a document for a BOW/unigram baseline\n",
    "# each feature is 'contains(keyword)' and is true or false depending\n",
    "# on whether that keyword is in the document\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a word and returns true if it consists only\n",
    "# of non-alphabetic characters  (assumes import re)\n",
    "def alpha_filter(w):\n",
    "    # pattern to match word of non-alphabetical characters\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if (pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features that include words as before \n",
    "# add the most frequent significant bigrams\n",
    "# this function takes the list of words in a document as an argument and returns a feature dictionary\n",
    "# it depends on the variables word_features and bigram_features\n",
    "def bigram_document_features(document, word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes a document list of words and returns a feature dictionary\n",
    "# it runs the default pos tagger (the Stanford tagger) on the document\n",
    "#   and counts 4 types of pos tags to use as features\n",
    "def POS_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross-validation ##\n",
    "# this function takes the number of folds, the feature sets\n",
    "# it iterates over the folds, using different sections for training and testing in turn\n",
    "#   it prints the precision, recall and F score for each fold\n",
    "#.  (it does not compute the average over the folds)\n",
    "def cross_validation_PRF(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    #accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round to produce the gold and predicted labels\n",
    "        goldlist = []\n",
    "        predictedlist = []\n",
    "        for (features, label) in test_this_round:\n",
    "            goldlist.append(label)\n",
    "            predictedlist.append(classifier.classify(features))\n",
    "\n",
    "        # call the function with our data\n",
    "        eval_measures(goldlist, predictedlist)\n",
    "    # this version doesn't save measures and compute averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a dictionary where you can look up words and get back \n",
    "#     the four items of subjectivity information described above\n",
    "def readSubjectivity(path):\n",
    "    flexicon = open(path, 'r')\n",
    "    # initialize an empty dictionary\n",
    "    sldict = { }\n",
    "    for line in flexicon:\n",
    "        fields = line.split()   # default is to split on whitespace\n",
    "        # split each field on the '=' and keep the second part as the value\n",
    "        strength = fields[0].split(\"=\")[1]\n",
    "        word = fields[2].split(\"=\")[1]\n",
    "        posTag = fields[3].split(\"=\")[1]\n",
    "        stemmed = fields[4].split(\"=\")[1]\n",
    "        polarity = fields[5].split(\"=\")[1]\n",
    "        if (stemmed == 'y'):\n",
    "            isStemmed = True\n",
    "        else:\n",
    "            isStemmed = False\n",
    "        # put a dictionary entry with the word as the keyword\n",
    "        #     and a list of the other values\n",
    "        sldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "    return sldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features that include word counts of subjectivity words\n",
    "# negative feature will have number of weakly negative words +\n",
    "#    2 * number of strongly negative words\n",
    "# positive feature has similar definition\n",
    "#    not counting neutral words\n",
    "\n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':    \n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':  \n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':    \n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':  \n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('Label\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the number of folds, the feature sets\n",
    "# it iterates over the folds, using different sections for training and testing in turn\n",
    "#   it prints the accuracy for each fold and the average accuracy at the end\n",
    "def cross_validation_accuracy(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round and save accuracy\n",
    "        accuracy_this_round = nltk.classify.accuracy(classifier, test_this_round)\n",
    "        print (i, accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "    # find mean accuracy over all rounds\n",
    "    print ('mean accuracy', sum(accuracy_list) / num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One strategy with negation words is to negate the word following the negation word\n",
    "#   other strategies negate all words up to the next punctuation\n",
    "# Strategy is to go through the document words in order adding the word features,\n",
    "#   but if the word follows a negation words, change the feature to negated word\n",
    "# Start the feature set with all 2000 word features and 2000 Not word features set to false\n",
    "\n",
    "def NOT_features(document, word_features, negationwords):    \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function writeFeatureSets:\n",
    "# takes featuresets defined in the nltk and convert them to weka input csv file\n",
    "#    any feature value in the featuresets should not contain \",\", \"'\" or \" itself\n",
    "#    and write the file to the outpath location\n",
    "#    outpath should include the name of the csv file\n",
    "def writeFeatureSets(featuresets, outpath):\n",
    "    # open outpath for writing\n",
    "    f = open(outpath, 'w')\n",
    "    # get the feature names from the feature dictionary in the first featureset\n",
    "    featurenames = featuresets[0][0].keys()\n",
    "    # create the first line of the file as comma separated feature names\n",
    "    #    with the word class as the last feature name\n",
    "    featurenameline = ''\n",
    "    for featurename in featurenames:\n",
    "        # replace forbidden characters with text abbreviations\n",
    "        featurename = featurename.replace(',','CM')\n",
    "        featurename = featurename.replace(\"'\",\"DQ\")\n",
    "        featurename = featurename.replace('\"','QU')\n",
    "        featurenameline += featurename + ','\n",
    "    featurenameline += 'class'\n",
    "    # write this as the first line in the csv file\n",
    "    f.write(featurenameline)\n",
    "    f.write('\\n')\n",
    "    # convert each feature set to a line in the file with comma separated feature values,\n",
    "    # each feature value is converted to a string\n",
    "    #   for booleans this is the words true and false\n",
    "    #   for numbers, this is the string with the number\n",
    "    for featureset in featuresets:\n",
    "        featureline = ''\n",
    "        for key in featurenames:\n",
    "            featureline += str(featureset[0][key]) + ','\n",
    "        featureline += featureset[1]\n",
    "        # write each feature set values to the file\n",
    "        f.write(featureline)\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features (keywords) of a document for a BOW/unigram baseline\n",
    "# each feature is the TF-IDF score if word is in document\n",
    "def TFIDF_document_features(document, TF_IDF_dict):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for k, v in topkeysnormalized:\n",
    "        if (k in document_words):\n",
    "            features['V_{}'.format(k)] = v\n",
    "        else:\n",
    "            features['V_{}'.format(k)] = 0\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dirPath = 'C:\\\\Users\\\\brcro\\\\OneDrive\\\\Documents\\\\Syracuse\\\\IST 664 - Natural Language Processing\\\\Final Project\\\\FinalProjectData\\\\EmailSpamCorpora\\\\corpus'\n",
    "dirPath = 'C:\\\\Users\\\\Brandon Croarkin\\\\Documents\\\\Education\\\\Syracuse\\\\NLP\\\\Final Project\\\\FinalProjectData\\\\EmailSpamCorpora\\\\corpus'\n",
    "limitStr = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam files: 1500\n",
      "Number of ham files: 1500\n"
     ]
    }
   ],
   "source": [
    "# convert the limit argument from a string to an int\n",
    "limit = int(limitStr)\n",
    "  \n",
    "# start lists for spam and ham email texts\n",
    "hamtexts = []\n",
    "spamtexts = []\n",
    "os.chdir(dirPath)\n",
    "# process all files in directory that end in .txt up to the limit\n",
    "#    assuming that the emails are sufficiently randomized\n",
    "for file in os.listdir(\"./spam\"):\n",
    "    if (file.endswith(\".txt\")) and (len(spamtexts) < limit):\n",
    "        # open file for reading and read entire file into a string\n",
    "        f = open(\"./spam/\"+file, 'r', encoding=\"latin-1\")\n",
    "        spamtexts.append (f.read())\n",
    "        f.close()\n",
    "for file in os.listdir(\"./ham\"):\n",
    "    if (file.endswith(\".txt\")) and (len(hamtexts) < limit):\n",
    "        # open file for reading and read entire file into a string\n",
    "        f = open(\"./ham/\"+file, 'r', encoding=\"latin-1\")\n",
    "        hamtexts.append (f.read())\n",
    "        f.close()\n",
    "\n",
    "# print number emails read\n",
    "print (\"Number of spam files:\",len(spamtexts))\n",
    "print (\"Number of ham files:\",len(hamtexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Subject', ':', 'entex', 'for', 'jan', '.', '2000', 'this', 'spreadsheet', 'shows', 'the', 'current', 'estimates', 'are', 'approximately', '12', ',', '000', '/', 'day', 'short', 'of', 'what', 'the', 'estimates', 'would', 'be', 'with', 'the', 'revised', 'entex', 'factor', '.', 'call', 'me', 'if', 'you', 'have', 'any', 'questions', '.', 'i', 'will', 'work', 'to', 'get', 'the', 'revised', 'estimates', 'into', 'pops', 'today', '.', 'gary'], 'ham')\n"
     ]
    }
   ],
   "source": [
    "# create list of mixed spam and ham email documents as (list of words, label)\n",
    "emaildocs = []\n",
    "# add all the spam\n",
    "for spam in spamtexts:\n",
    "    tokens = nltk.word_tokenize(spam)\n",
    "    emaildocs.append((tokens, 'spam'))\n",
    "# add all the regular emails\n",
    "for ham in hamtexts:\n",
    "    tokens = nltk.word_tokenize(ham)\n",
    "    emaildocs.append((tokens, 'ham'))\n",
    "    \n",
    "# randomize the list\n",
    "random.shuffle(emaildocs)\n",
    "  \n",
    "# print a few token lists\n",
    "for email in emaildocs[:1]:\n",
    "    print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '.', '/', ',', ':', 'the', 'to', 'ect', 'and', 'of', '@', 'a', 'for', '?', 'you', 'in', 'this', 'is', 'hou', 'on', 'i', \"'\", ')', '=', '(', 'enron', 'Subject', '!', 'be', 'your', '2000', 'that', 'with', 'from', '_', 'will', 'have', 'we', 's', 'as', 'are', 'it', '$', '>', 'or', '3', 'at', 'not', 'by', 'please']\n"
     ]
    }
   ],
   "source": [
    "# get all words from all emails and put into a frequency \n",
    "#   note lowercase, but no stemming or stopwords\n",
    "all_words_list = [word for (sent,cat) in emaildocs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "\n",
    "# get the 2000 most frequently appearing keywords in the corpus\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word,count) in word_items]\n",
    "print(word_features[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Negation Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this list of negation words includes some \"approximate negators\" like hardly and rarely\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '.', '/', ',', ':', 'ect', '@', '?', 'hou', \"'\", ')', '=', '(', 'enron', 'Subject', '!', '2000', '_', '$', '>', '3', 'not', 'please', '``', 'com', '|', '1', ';', '#', '2']\n"
     ]
    }
   ],
   "source": [
    "new_all_words_list = [word for word in all_words_list if word not in newstopwords]\n",
    "new_all_words = nltk.FreqDist(new_all_words_list)\n",
    "new_word_items = new_all_words.most_common(2000)\n",
    "negation_word_features = [word for (word,count) in new_word_items]\n",
    "print(negation_word_features[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newstopwords = [word for word in stopwords if word not in negationwords]\n",
    "len(newstopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_words_list = [word for word in all_words_list if word not in newstopwords]\n",
    "new_all_words = nltk.FreqDist(new_all_words_list)\n",
    "new_word_items = new_all_words.most_common(2000)\n",
    "no_stop_word_features = [word for (word,count) in new_word_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '.', '/', ',', ':', 'ect', '@', '?', 'hou', \"'\", ')', '=', '(', 'enron', 'Subject', '!', '2000', '_', '$', '>', '3', 'not', 'please', '``', 'com', '|', '1', ';', '#', '2']\n"
     ]
    }
   ],
   "source": [
    "print(no_stop_word_features[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Non-Alpha Characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject', 'entex', 'for', 'jan', 'this', 'spreadsheet', 'shows', 'the', 'current', 'estimates', 'are', 'approximately', 'day', 'short', 'of', 'what', 'the', 'estimates', 'would', 'be', 'with', 'the', 'revised', 'entex', 'factor', 'call', 'me', 'if', 'you', 'have', 'any', 'questions', 'i', 'will', 'work', 'to', 'get', 'the', 'revised', 'estimates', 'into', 'pops', 'today', 'gary', 'Subject', 'nomination', 'tuesday', 'this', 'is', 'to', 'nominate', 'mmbtu', 'd', 'for', 'into', 'eastrans', 'mmbtu', 'd', 'will', 'be', 'redelivered', 'into', 'pg', 'e', 'the', 'rest', 'into', 'hpl', 'from', 'fuels', 'cotton', 'valley', 'we', 'plan', 'to', 'begin', 'receiving', 'the', 'gas', 'at', 'pm', 'or', 'so', 'and', 'take', 'more', 'volume', 'to', 'average', 'the', 'nominated', 'quantity', 'for', 'the', 'day', 'as', 'i', 'indicated', 'on', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Filtered word feature\n",
    "filtered_words_list = [word for (sent, cat) in emaildocs for word in sent if not alpha_filter(word)]\n",
    "print(filtered_words_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41065\n"
     ]
    }
   ],
   "source": [
    "filtered_words = nltk.FreqDist(filtered_words_list)\n",
    "print(len(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_items = filtered_words.most_common(1500)\n",
    "filtered_word_features = [word for (word,count) in filtered_word_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Unigram Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use document_features function to create unigram feature sets for all sentences\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9596296296296296"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a classifier and report accuracy\n",
    "train_set, test_set = featuresets[2700:], featuresets[:2700]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  V_2000 = True              ham : spam   =     20.7 : 1.0\n",
      "                  V_http = True             spam : ham    =     20.5 : 1.0\n",
      "                    V_pm = True              ham : spam   =     19.9 : 1.0\n",
      "             V_questions = True              ham : spam   =     18.8 : 1.0\n",
      "                V_volume = True              ham : spam   =     16.8 : 1.0\n",
      "                   V_gas = True              ham : spam   =     14.8 : 1.0\n",
      "                     V_* = True             spam : ham    =     14.5 : 1.0\n",
      "                   V_ami = True              ham : spam   =     14.1 : 1.0\n",
      "                  V_file = True              ham : spam   =     14.1 : 1.0\n",
      "                 V_offer = True             spam : ham    =     13.8 : 1.0\n",
      "                    V_07 = True              ham : spam   =     13.5 : 1.0\n",
      "                    V_06 = True              ham : spam   =     12.4 : 1.0\n",
      "                  V_many = True             spam : ham    =     11.1 : 1.0\n",
      "                    V_28 = True              ham : spam   =     10.9 : 1.0\n",
      "                     V_. = False            spam : ham    =     10.7 : 1.0\n",
      "                   V_www = True             spam : ham    =     10.7 : 1.0\n",
      "                 V_cheap = True             spam : ham    =      9.8 : 1.0\n",
      "               V_tickets = True              ham : spam   =      9.5 : 1.0\n",
      "                 V_smith = True              ham : spam   =      9.5 : 1.0\n",
      "                  V_deal = True              ham : spam   =      9.5 : 1.0\n",
      "                     V_& = True              ham : spam   =      9.4 : 1.0\n",
      "                  V_corp = True              ham : spam   =      9.4 : 1.0\n",
      "                  V_july = True              ham : spam   =      9.3 : 1.0\n",
      "                  V_save = True             spam : ham    =      9.1 : 1.0\n",
      "                  V_home = True             spam : ham    =      9.1 : 1.0\n",
      "                V_prices = True             spam : ham    =      8.7 : 1.0\n",
      "                     V_n = True             spam : ham    =      8.7 : 1.0\n",
      "                    V_09 = True              ham : spam   =      8.5 : 1.0\n",
      "                  V_link = True             spam : ham    =      8.4 : 1.0\n",
      "               V_receive = True             spam : ham    =      8.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Unigram with Non-Alpha Characters Removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d, filtered_word_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9574074074074074"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_set, new_test_set = featuresets[2700:], featuresets[:2700]\n",
    "classifier1 = nltk.NaiveBayesClassifier.train(new_train_set)\n",
    "nltk.classify.accuracy(classifier1, new_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 600\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "spam \t      0.981      0.943      0.962\n",
      "ham \t      0.932      0.978      0.954\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.935      0.997      0.965\n",
      "spam \t      0.997      0.935      0.965\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.936      0.993      0.964\n",
      "spam \t      0.993      0.940      0.966\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.933      1.000      0.966\n",
      "spam \t      1.000      0.931      0.964\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.943      0.996      0.969\n",
      "spam \t      0.997      0.947      0.971\n"
     ]
    }
   ],
   "source": [
    "# train classifier and show performance in cross-validation\n",
    "num_folds = 5\n",
    "cross_validation_PRF(num_folds, featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Unigram with Negation Words Removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create feature set with Negation Word Features\n",
    "new_NOT_featuresets = [(NOT_features(d, negation_word_features, negationwords), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9688888888888889"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_set, new_test_set = new_NOT_featuresets[2700:], new_NOT_featuresets[:2700]\n",
    "classifier1 = nltk.NaiveBayesClassifier.train(new_train_set)\n",
    "nltk.classify.accuracy(classifier1, new_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  V_2000 = True              ham : spam   =     20.7 : 1.0\n",
      "                  V_http = True             spam : ham    =     20.5 : 1.0\n",
      "                    V_pm = True              ham : spam   =     19.9 : 1.0\n",
      "             V_questions = True              ham : spam   =     18.8 : 1.0\n",
      "                V_volume = True              ham : spam   =     16.8 : 1.0\n",
      "                   V_gas = True              ham : spam   =     14.8 : 1.0\n",
      "                     V_* = True             spam : ham    =     14.5 : 1.0\n",
      "                   V_ami = True              ham : spam   =     14.1 : 1.0\n",
      "                  V_file = True              ham : spam   =     14.1 : 1.0\n",
      "                 V_offer = True             spam : ham    =     13.8 : 1.0\n",
      "                    V_07 = True              ham : spam   =     13.5 : 1.0\n",
      "                    V_06 = True              ham : spam   =     12.4 : 1.0\n",
      "                  V_many = True             spam : ham    =     11.1 : 1.0\n",
      "                    V_28 = True              ham : spam   =     10.9 : 1.0\n",
      "                     V_. = False            spam : ham    =     10.7 : 1.0\n",
      "                   V_www = True             spam : ham    =     10.7 : 1.0\n",
      "                 V_cheap = True             spam : ham    =      9.8 : 1.0\n",
      "               V_tickets = True              ham : spam   =      9.5 : 1.0\n",
      "                 V_smith = True              ham : spam   =      9.5 : 1.0\n",
      "                  V_deal = True              ham : spam   =      9.5 : 1.0\n",
      "                     V_& = True              ham : spam   =      9.4 : 1.0\n",
      "                  V_corp = True              ham : spam   =      9.4 : 1.0\n",
      "                  V_july = True              ham : spam   =      9.3 : 1.0\n",
      "                  V_home = True             spam : ham    =      9.1 : 1.0\n",
      "                  V_save = True             spam : ham    =      9.1 : 1.0\n",
      "                     V_n = True             spam : ham    =      8.7 : 1.0\n",
      "                V_prices = True             spam : ham    =      8.7 : 1.0\n",
      "                    V_09 = True              ham : spam   =      8.5 : 1.0\n",
      "               V_receive = True             spam : ham    =      8.4 : 1.0\n",
      "            V_investment = True             spam : ham    =      8.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier1.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Unigram with Stop Words Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use document_features function to create unigram feature sets for all sentences\n",
    "# using word features with stop words removed\n",
    "featuresets = [(document_features(d, no_stop_word_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9622222222222222"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a classifier and report accuracy\n",
    "train_set, test_set = featuresets[2700:], featuresets[:2700]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  V_2000 = True              ham : spam   =     20.7 : 1.0\n",
      "                  V_http = True             spam : ham    =     20.5 : 1.0\n",
      "                    V_pm = True              ham : spam   =     19.9 : 1.0\n",
      "             V_questions = True              ham : spam   =     18.8 : 1.0\n",
      "                V_volume = True              ham : spam   =     16.8 : 1.0\n",
      "                   V_gas = True              ham : spam   =     14.8 : 1.0\n",
      "                     V_* = True             spam : ham    =     14.5 : 1.0\n",
      "                   V_ami = True              ham : spam   =     14.1 : 1.0\n",
      "                  V_file = True              ham : spam   =     14.1 : 1.0\n",
      "                 V_offer = True             spam : ham    =     13.8 : 1.0\n",
      "                    V_07 = True              ham : spam   =     13.5 : 1.0\n",
      "                    V_06 = True              ham : spam   =     12.4 : 1.0\n",
      "                  V_many = True             spam : ham    =     11.1 : 1.0\n",
      "                    V_28 = True              ham : spam   =     10.9 : 1.0\n",
      "                     V_. = False            spam : ham    =     10.7 : 1.0\n",
      "                   V_www = True             spam : ham    =     10.7 : 1.0\n",
      "                 V_cheap = True             spam : ham    =      9.8 : 1.0\n",
      "               V_tickets = True              ham : spam   =      9.5 : 1.0\n",
      "                 V_smith = True              ham : spam   =      9.5 : 1.0\n",
      "                  V_deal = True              ham : spam   =      9.5 : 1.0\n",
      "                     V_& = True              ham : spam   =      9.4 : 1.0\n",
      "                  V_corp = True              ham : spam   =      9.4 : 1.0\n",
      "                  V_july = True              ham : spam   =      9.3 : 1.0\n",
      "                  V_save = True             spam : ham    =      9.1 : 1.0\n",
      "                  V_home = True             spam : ham    =      9.1 : 1.0\n",
      "                V_prices = True             spam : ham    =      8.7 : 1.0\n",
      "                     V_n = True             spam : ham    =      8.7 : 1.0\n",
      "                    V_09 = True              ham : spam   =      8.5 : 1.0\n",
      "                  V_link = True             spam : ham    =      8.4 : 1.0\n",
      "               V_receive = True             spam : ham    =      8.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Unigram with Negation Features and Stop Words Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding Negation features to word features with stop words removed\n",
    "new_NOT_featuresets = [(NOT_features(d, new_word_features, negationwords), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9688888888888889"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_set, new_test_set = new_NOT_featuresets[2700:], new_NOT_featuresets[:2700]\n",
    "classifier1 = nltk.NaiveBayesClassifier.train(new_train_set)\n",
    "nltk.classify.accuracy(classifier1, new_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Unigram with Larger Featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 6000 most frequently appearing keywords in the corpus\n",
    "word_items_large = all_words.most_common(6000)\n",
    "word_features_large = [word for (word,count) in word_items_large]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use document_features function to create unigram feature sets for all sentences\n",
    "featuresets_large = [(document_features(d, word_features_large), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "#print the number of features to confirm that there are 6000 now\n",
    "print(len(featuresets_large))\n",
    "print(len(featuresets_large[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a classifier and report accuracy\n",
    "train_set, test_set = featuresets_large[2700:], featuresets_large[:2700]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Bigram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create short cut variable name for the bigram association measure\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "#Create bigram collocation finder using all_words_list\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "#Use raw_freq to get bigrams\n",
    "bigram_features = finder.nbest(bigram_measures.raw_freq, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to create feature sets for all sentences\n",
    "bigram_featuresets = [(bigram_document_features(d, word_features, bigram_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "# number of features for document 0\n",
    "print(len(bigram_featuresets[0][0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'V_-': False,\n",
       " 'V_.': True,\n",
       " 'V_/': True,\n",
       " 'V_,': True,\n",
       " 'V_:': True,\n",
       " 'V_the': True,\n",
       " 'V_to': True,\n",
       " 'V_ect': False,\n",
       " 'V_and': False,\n",
       " 'V_of': True,\n",
       " 'V_@': False,\n",
       " 'V_a': False,\n",
       " 'V_for': True,\n",
       " 'V_?': False,\n",
       " 'V_you': True,\n",
       " 'V_in': False,\n",
       " 'V_this': True,\n",
       " 'V_is': False,\n",
       " 'V_hou': False,\n",
       " 'V_on': False,\n",
       " 'V_i': True,\n",
       " \"V_'\": False,\n",
       " 'V_)': False,\n",
       " 'V_=': False,\n",
       " 'V_(': False,\n",
       " 'V_enron': False,\n",
       " 'V_Subject': True,\n",
       " 'V_!': False,\n",
       " 'V_be': True,\n",
       " 'V_your': False,\n",
       " 'V_2000': True,\n",
       " 'V_that': False,\n",
       " 'V_with': True,\n",
       " 'V_from': False,\n",
       " 'V__': False,\n",
       " 'V_will': True,\n",
       " 'V_have': True,\n",
       " 'V_we': False,\n",
       " 'V_s': False,\n",
       " 'V_as': False,\n",
       " 'V_are': True,\n",
       " 'V_it': False,\n",
       " 'V_$': False,\n",
       " 'V_>': False,\n",
       " 'V_or': False,\n",
       " 'V_3': False,\n",
       " 'V_at': False,\n",
       " 'V_not': False,\n",
       " 'V_by': False,\n",
       " 'V_please': False,\n",
       " 'V_``': False,\n",
       " 'V_com': False,\n",
       " 'V_if': True,\n",
       " 'V_|': False,\n",
       " 'V_1': False,\n",
       " 'V_;': False,\n",
       " 'V_#': False,\n",
       " 'V_our': False,\n",
       " 'V_me': True,\n",
       " 'V_2': False,\n",
       " 'V_e': False,\n",
       " 'V_subject': False,\n",
       " 'V_gas': False,\n",
       " 'V_all': False,\n",
       " 'V_00': False,\n",
       " 'V_%': False,\n",
       " 'V_*': False,\n",
       " 'V_meter': False,\n",
       " 'V_am': False,\n",
       " 'V_can': False,\n",
       " 'V_any': True,\n",
       " 'V_cc': False,\n",
       " 'V_d': False,\n",
       " 'V_pm': False,\n",
       " 'V_000': True,\n",
       " 'V_deal': False,\n",
       " 'V_corp': False,\n",
       " 'V_http': False,\n",
       " 'V_has': False,\n",
       " 'V_no': False,\n",
       " 'V_an': False,\n",
       " 'V_0': False,\n",
       " 'V_re': False,\n",
       " 'V_4': False,\n",
       " 'V_10': False,\n",
       " 'V_new': False,\n",
       " 'V_hpl': False,\n",
       " 'V_company': False,\n",
       " 'V_5': False,\n",
       " 'V_was': False,\n",
       " 'V_thanks': False,\n",
       " 'V_up': False,\n",
       " 'V_7': False,\n",
       " 'V_get': True,\n",
       " 'V_t': False,\n",
       " 'V_99': False,\n",
       " 'V_&': False,\n",
       " 'V_know': False,\n",
       " 'V_information': False,\n",
       " 'V_may': False,\n",
       " 'V_forwarded': False,\n",
       " 'V_more': False,\n",
       " 'V_need': False,\n",
       " 'V_daren': False,\n",
       " 'V_do': False,\n",
       " 'V_12': True,\n",
       " 'V_here': False,\n",
       " 'V_only': False,\n",
       " 'V_out': False,\n",
       " 'V_price': False,\n",
       " 'V_these': False,\n",
       " 'V_there': False,\n",
       " 'V_one': False,\n",
       " 'V_us': False,\n",
       " 'V_my': False,\n",
       " 'V_p': False,\n",
       " 'V_time': False,\n",
       " 'V_www': False,\n",
       " 'V_into': True,\n",
       " 'V_03': False,\n",
       " 'V_l': False,\n",
       " 'V_should': False,\n",
       " 'V_6': False,\n",
       " 'V_been': False,\n",
       " 'V_8': False,\n",
       " 'V_mmbtu': False,\n",
       " 'V_but': False,\n",
       " 'V_email': False,\n",
       " 'V_would': True,\n",
       " 'V_m': False,\n",
       " 'V_11': False,\n",
       " 'V_let': False,\n",
       " 'V_see': False,\n",
       " 'V_01': False,\n",
       " 'V_02': False,\n",
       " 'V_b': False,\n",
       " 'V_day': True,\n",
       " 'V_so': False,\n",
       " 'V_what': True,\n",
       " 'V_which': False,\n",
       " 'V_th': False,\n",
       " 'V_08': False,\n",
       " 'V_j': False,\n",
       " 'V_now': False,\n",
       " 'V_robert': False,\n",
       " 'V_20': False,\n",
       " 'V_font': False,\n",
       " 'V_mail': False,\n",
       " 'V_04': False,\n",
       " 'V_td': False,\n",
       " 'V_sitara': False,\n",
       " 'V_report': False,\n",
       " 'V_about': False,\n",
       " 'V_statements': False,\n",
       " 'V_month': False,\n",
       " 'V_05': False,\n",
       " 'V_o': False,\n",
       " 'V_their': False,\n",
       " 'V_06': False,\n",
       " 'V_they': False,\n",
       " 'V_july': False,\n",
       " 'V_15': False,\n",
       " 'V_other': False,\n",
       " 'V_9': False,\n",
       " 'V_attached': False,\n",
       " 'V_30': False,\n",
       " 'V_x': False,\n",
       " 'V_also': False,\n",
       " 'V_contract': False,\n",
       " 'V_just': False,\n",
       " 'V_+': False,\n",
       " 'V_energy': False,\n",
       " 'V_over': False,\n",
       " 'V_like': False,\n",
       " 'V_xls': False,\n",
       " 'V_free': False,\n",
       " 'V_farmer': False,\n",
       " 'V_nbsp': False,\n",
       " 'V_its': False,\n",
       " 'V_inc': False,\n",
       " 'V_volume': False,\n",
       " 'V_deals': False,\n",
       " 'V_some': False,\n",
       " 'V_business': False,\n",
       " 'V_message': False,\n",
       " 'V_09': False,\n",
       " 'V_when': False,\n",
       " 'V_c': False,\n",
       " 'V_want': False,\n",
       " 'V_questions': True,\n",
       " 'V_07': False,\n",
       " 'V_r': False,\n",
       " 'V_contact': False,\n",
       " 'V_make': False,\n",
       " 'V_change': False,\n",
       " 'V_volumes': False,\n",
       " 'V_within': False,\n",
       " 'V_25': False,\n",
       " 'V_height': False,\n",
       " 'V_production': False,\n",
       " 'V_call': True,\n",
       " 'V_[': False,\n",
       " 'V_]': False,\n",
       " 'V_100': False,\n",
       " 'V_well': False,\n",
       " 'V_could': False,\n",
       " 'V_stock': False,\n",
       " 'V_nom': False,\n",
       " 'V_ami': False,\n",
       " 'V_who': False,\n",
       " 'V_forward': False,\n",
       " 'V_back': False,\n",
       " 'V_31': False,\n",
       " 'V_gary': True,\n",
       " 'V_today': True,\n",
       " 'V_he': False,\n",
       " 'V_21': False,\n",
       " 'V_line': False,\n",
       " 'V_24': False,\n",
       " 'V_number': False,\n",
       " 'V_list': False,\n",
       " 'V_how': False,\n",
       " 'V_ticket': False,\n",
       " 'V_following': False,\n",
       " 'V_through': False,\n",
       " 'V_16': False,\n",
       " 'V_go': False,\n",
       " 'V_money': False,\n",
       " 'V_order': False,\n",
       " 'V_pills': False,\n",
       " 'V_size': False,\n",
       " 'V_them': False,\n",
       " 'V_oil': False,\n",
       " 'V_50': False,\n",
       " 'V_width': False,\n",
       " 'V_were': False,\n",
       " 'V_best': False,\n",
       " 'V_set': False,\n",
       " 'V_click': False,\n",
       " 'V_below': False,\n",
       " 'V_take': False,\n",
       " 'V_2004': False,\n",
       " 'V_first': False,\n",
       " 'V_texas': False,\n",
       " 'V_net': False,\n",
       " 'V_u': False,\n",
       " 'V_investment': False,\n",
       " 'V_per': False,\n",
       " 'V_f': False,\n",
       " 'V_online': False,\n",
       " 'V_looking': False,\n",
       " 'V_pat': False,\n",
       " 'V_use': False,\n",
       " 'V_w': False,\n",
       " 'V_days': False,\n",
       " 'V_60': False,\n",
       " 'V_available': False,\n",
       " 'V_don': False,\n",
       " 'V_system': False,\n",
       " 'V_products': False,\n",
       " 'V_file': False,\n",
       " 'V_than': False,\n",
       " 'V_america': False,\n",
       " 'V_june': False,\n",
       " 'V_flow': False,\n",
       " 'V_ena': False,\n",
       " 'V_securities': False,\n",
       " 'V_future': False,\n",
       " 'V_sent': False,\n",
       " 'V_next': False,\n",
       " 'V_north': False,\n",
       " 'V_sale': False,\n",
       " 'V_product': False,\n",
       " 'V_management': False,\n",
       " 'V_effective': False,\n",
       " 'V_chokshi': False,\n",
       " 'V_28': False,\n",
       " 'V_had': False,\n",
       " 'V_before': False,\n",
       " 'V_service': False,\n",
       " 'V_help': False,\n",
       " 'V_group': False,\n",
       " 'V_prices': False,\n",
       " 'V_made': False,\n",
       " 'V_she': False,\n",
       " 'V_14': False,\n",
       " 'V_services': False,\n",
       " 'V_lloyd': False,\n",
       " 'V_many': False,\n",
       " 'V_each': False,\n",
       " 'V_office': False,\n",
       " 'V_k': False,\n",
       " 'V_based': False,\n",
       " 'V_g': False,\n",
       " 'V_last': False,\n",
       " 'V_after': False,\n",
       " 'V_23': False,\n",
       " 'V_pec': False,\n",
       " 'V_n': False,\n",
       " 'V_market': False,\n",
       " 'V_22': False,\n",
       " 'V_17': False,\n",
       " 'V_v': False,\n",
       " 'V_computron': False,\n",
       " 'V_27': False,\n",
       " 'V_internet': False,\n",
       " 'V_such': False,\n",
       " 'V_align': False,\n",
       " 'V_save': False,\n",
       " 'V_nomination': False,\n",
       " 'V_send': False,\n",
       " 'V_most': False,\n",
       " 'V_two': False,\n",
       " 'V_account': False,\n",
       " 'V_h': False,\n",
       " 'V_news': False,\n",
       " 'V_without': False,\n",
       " 'V_jackie': False,\n",
       " 'V_link': False,\n",
       " 'V_daily': False,\n",
       " 'V_address': False,\n",
       " 'V_high': False,\n",
       " 'V_18': False,\n",
       " 'V_activity': False,\n",
       " 'V_year': False,\n",
       " 'V_name': False,\n",
       " 'V_work': True,\n",
       " 'V_tr': False,\n",
       " 'V_march': False,\n",
       " 'V_info': False,\n",
       " 'V_his': False,\n",
       " 'V_due': False,\n",
       " 'V_changes': False,\n",
       " 'V_does': False,\n",
       " 'V_sales': False,\n",
       " 'V_week': False,\n",
       " 'V_january': False,\n",
       " 'V_being': False,\n",
       " 'V_bob': False,\n",
       " 'V_software': False,\n",
       " 'V_then': False,\n",
       " 'V_george': False,\n",
       " 'V_very': False,\n",
       " 'V_companies': False,\n",
       " 'V_special': False,\n",
       " 'V_thank': False,\n",
       " 'V_steve': False,\n",
       " 'V_same': False,\n",
       " 'V_29': False,\n",
       " 'V_off': False,\n",
       " 'V_plant': False,\n",
       " 'V_million': False,\n",
       " 'V_august': False,\n",
       " \"V_''\": False,\n",
       " 'V_microsoft': False,\n",
       " 'V_request': False,\n",
       " 'V_13': False,\n",
       " 'V_windows': False,\n",
       " 'V_19': False,\n",
       " 'V_international': False,\n",
       " 'V_team': False,\n",
       " 'V_transport': False,\n",
       " 'V_color': False,\n",
       " 'V_98': False,\n",
       " 'V_because': False,\n",
       " 'V_offer': False,\n",
       " 'V_world': False,\n",
       " 'V_april': False,\n",
       " 'V_less': False,\n",
       " 'V_above': False,\n",
       " 'V_melissa': False,\n",
       " 'V_69': False,\n",
       " 'V_site': False,\n",
       " 'V_good': False,\n",
       " 'V_face': False,\n",
       " 'V_reply': False,\n",
       " 'V_stop': False,\n",
       " 'V_buy': False,\n",
       " 'V_border': False,\n",
       " 'V_find': False,\n",
       " 'V_clynes': False,\n",
       " 'V_date': False,\n",
       " 'V_look': False,\n",
       " 'V_process': False,\n",
       " 'V_95': False,\n",
       " 'V_under': False,\n",
       " 'V_40': False,\n",
       " 'V_90': False,\n",
       " 'V_performance': False,\n",
       " 'V_professional': False,\n",
       " 'V_agreement': False,\n",
       " 'V_check': False,\n",
       " 'V_susan': False,\n",
       " 'V_give': False,\n",
       " 'V_href': False,\n",
       " 'V_smith': False,\n",
       " 'V_mary': False,\n",
       " 'V_\\x01': False,\n",
       " 'V_those': False,\n",
       " 'V_people': False,\n",
       " 'V_nd': False,\n",
       " 'V_did': False,\n",
       " 'V_visit': False,\n",
       " 'V_down': False,\n",
       " 'V_hours': False,\n",
       " 'V_home': False,\n",
       " 'V_where': False,\n",
       " 'V_viagra': False,\n",
       " 'V_needs': False,\n",
       " 'V_desk': False,\n",
       " 'V_ll': False,\n",
       " 'V_again': False,\n",
       " 'V_long': False,\n",
       " 'V_receive': False,\n",
       " 'V_advice': False,\n",
       " 'V_br': False,\n",
       " 'V_total': False,\n",
       " 'V_both': False,\n",
       " 'V_said': False,\n",
       " 'V_data': False,\n",
       " 'V_purchase': False,\n",
       " 'V_45': False,\n",
       " 'V_vance': False,\n",
       " 'V_26': False,\n",
       " 'V_act': False,\n",
       " 'V_risk': False,\n",
       " 'V_st': False,\n",
       " 'V_phone': False,\n",
       " 'V_needed': False,\n",
       " 'V_development': False,\n",
       " 'V_lisa': False,\n",
       " 'V_co': False,\n",
       " 'V_delivery': False,\n",
       " 'V_marketing': False,\n",
       " 'V_hplc': False,\n",
       " 'V_great': False,\n",
       " 'V_her': False,\n",
       " 'V_currently': False,\n",
       " 'V_provide': False,\n",
       " 'V_remove': False,\n",
       " 'V_further': False,\n",
       " 'V_via': False,\n",
       " 'V_place': False,\n",
       " 'V_section': False,\n",
       " 'V_security': False,\n",
       " 'V_results': False,\n",
       " 'V_xp': False,\n",
       " 'V_right': False,\n",
       " 'V_stocks': False,\n",
       " 'V_howard': False,\n",
       " 'V_center': False,\n",
       " 'V_ve': False,\n",
       " 'V_corporation': False,\n",
       " 'V_adobe': False,\n",
       " 'V_meeting': False,\n",
       " 'V_every': False,\n",
       " 'V_young': False,\n",
       " 'V_tickets': False,\n",
       " 'V_aimee': False,\n",
       " 'V_src': False,\n",
       " 'V_jan': True,\n",
       " 'V_own': False,\n",
       " 'V_sure': False,\n",
       " 'V_low': False,\n",
       " 'V_additional': False,\n",
       " 'V_note': False,\n",
       " 'V_since': False,\n",
       " 'V_pt': False,\n",
       " 'V_part': False,\n",
       " 'V_feb': False,\n",
       " 'V_80': False,\n",
       " 'V_must': False,\n",
       " 'V_much': False,\n",
       " 'V_revised': True,\n",
       " 'V_keep': False,\n",
       " 'V_teco': False,\n",
       " 'V_rates': False,\n",
       " 'V_500': False,\n",
       " 'V_going': False,\n",
       " 'V_show': False,\n",
       " 'V_global': False,\n",
       " 'V_come': False,\n",
       " 'V_quality': False,\n",
       " 'V_63': False,\n",
       " 'V_sell': False,\n",
       " 'V_trading': False,\n",
       " 'V_120': False,\n",
       " 'V_hanks': False,\n",
       " 'V_offers': False,\n",
       " 'V_even': False,\n",
       " 'V_friday': False,\n",
       " 'V_cialis': False,\n",
       " 'V_operations': False,\n",
       " 'V_still': False,\n",
       " 'V_point': False,\n",
       " 'V_think': False,\n",
       " 'V_pipeline': False,\n",
       " 'V_used': False,\n",
       " 'V_anita': False,\n",
       " 'V_copy': False,\n",
       " 'V_brenda': False,\n",
       " 'V_issue': False,\n",
       " 'V_unify': False,\n",
       " 'V_end': False,\n",
       " 'V_fuel': False,\n",
       " 'V_713': False,\n",
       " 'V_pg': False,\n",
       " 'V_way': False,\n",
       " 'V_numbers': False,\n",
       " 'V_works': False,\n",
       " 'V_camp': False,\n",
       " 'V_tom': False,\n",
       " 'V_financial': False,\n",
       " 'V_once': False,\n",
       " 'V_pay': False,\n",
       " 'V_fax': False,\n",
       " 'V_position': False,\n",
       " 'V_meters': False,\n",
       " 'V_current': True,\n",
       " 'V_y': False,\n",
       " 'V_support': False,\n",
       " 'V_full': False,\n",
       " 'V_prior': False,\n",
       " 'V_transaction': False,\n",
       " 'V_44': False,\n",
       " 'V_hi': False,\n",
       " 'V_believe': False,\n",
       " 'V_shares': False,\n",
       " 'V_fyi': False,\n",
       " 'V_graves': False,\n",
       " 'V_natural': False,\n",
       " 'V_dollars': False,\n",
       " 'V_using': False,\n",
       " 'V_counterparty': False,\n",
       " 'V_rate': False,\n",
       " 'V_trade': False,\n",
       " 'V_feel': False,\n",
       " 'V_details': False,\n",
       " 'V_hplo': False,\n",
       " 'V_soft': False,\n",
       " 'V_161': False,\n",
       " 'V_months': False,\n",
       " 'V_index': False,\n",
       " 'V_2003': False,\n",
       " 'V_web': False,\n",
       " 'V_increase': False,\n",
       " 'V_interest': False,\n",
       " 'V_notice': False,\n",
       " 'V_newsletter': False,\n",
       " 'V_customers': False,\n",
       " 'V_during': False,\n",
       " 'V_julie': False,\n",
       " 'V_eastrans': False,\n",
       " 'V_id': False,\n",
       " 'V_include': False,\n",
       " 'V_family': False,\n",
       " 'V_access': False,\n",
       " 'V_customer': False,\n",
       " 'V_html': False,\n",
       " 'V_control': False,\n",
       " 'V_prescription': False,\n",
       " 'V_including': False,\n",
       " 'V_review': False,\n",
       " 'V_noms': False,\n",
       " 'V_limited': False,\n",
       " 'V_read': False,\n",
       " 'V_problem': False,\n",
       " 'V_taylor': False,\n",
       " 'V_top': False,\n",
       " 'V_monday': False,\n",
       " 'V_resources': False,\n",
       " 'V_meds': False,\n",
       " 'V_project': False,\n",
       " 'V_cd': False,\n",
       " 'V_content': False,\n",
       " 'V_michael': False,\n",
       " 'V_while': False,\n",
       " 'V_until': False,\n",
       " 'V_tap': False,\n",
       " 'V_party': False,\n",
       " 'V_someone': False,\n",
       " 'V_style': False,\n",
       " 'V_real': False,\n",
       " 'V_why': False,\n",
       " 'V_term': False,\n",
       " 'V_put': False,\n",
       " 'V_32': False,\n",
       " 'V_spot': False,\n",
       " 'V_lee': False,\n",
       " 'V_february': False,\n",
       " 'V_cost': False,\n",
       " 'V_`': False,\n",
       " 'V_power': False,\n",
       " 'V_fact': False,\n",
       " 'V_three': False,\n",
       " 'V_past': False,\n",
       " 'V_type': False,\n",
       " 'V_provided': False,\n",
       " 'V_life': False,\n",
       " 'V_allocation': False,\n",
       " 'V_brian': False,\n",
       " 'V_better': False,\n",
       " 'V_albrecht': False,\n",
       " 'V_issues': False,\n",
       " 'V_technology': False,\n",
       " 'V_herod': False,\n",
       " 'V_received': False,\n",
       " 'V_created': False,\n",
       " 'V_pops': True,\n",
       " 'V_start': False,\n",
       " 'V_always': False,\n",
       " 'V_36': False,\n",
       " 'V_coastal': False,\n",
       " 'V_paliourg': False,\n",
       " 'V_soon': False,\n",
       " 'V_1999': False,\n",
       " 'V_contracts': False,\n",
       " 'V_stephanie': False,\n",
       " 'V_possible': False,\n",
       " 'V_investing': False,\n",
       " 'V_never': False,\n",
       " 'V_de': False,\n",
       " 'V_tx': False,\n",
       " 'V_error': False,\n",
       " 'V_entered': False,\n",
       " 'V_accounting': False,\n",
       " 'V_got': False,\n",
       " 'V_storage': False,\n",
       " 'V_cash': False,\n",
       " 'V_gco': False,\n",
       " 'V_done': False,\n",
       " 'V_iferc': False,\n",
       " 'V_move': False,\n",
       " 'V_35': False,\n",
       " 'V_ces': False,\n",
       " 'V_buyback': False,\n",
       " 'V_allocated': False,\n",
       " 'V_working': False,\n",
       " 'V_able': False,\n",
       " 'V_luong': False,\n",
       " 'V_pro': False,\n",
       " 'V_ms': False,\n",
       " 'V_voip': False,\n",
       " 'V_necessary': False,\n",
       " 'V_industry': False,\n",
       " 'V_continue': False,\n",
       " 'V_shipping': False,\n",
       " 'V_actuals': False,\n",
       " 'V_800': False,\n",
       " 'V_events': False,\n",
       " 'V_wish': False,\n",
       " 'V_houston': False,\n",
       " 'V_hsc': False,\n",
       " 'V_strong': False,\n",
       " 'V_fred': False,\n",
       " 'V_follow': False,\n",
       " 'V_release': False,\n",
       " 'V_lst': False,\n",
       " 'V_little': False,\n",
       " 'V_estimates': True,\n",
       " 'V_easy': False,\n",
       " 'V_program': False,\n",
       " 'V_manager': False,\n",
       " 'V_allen': False,\n",
       " 'V_scott': False,\n",
       " 'V_ls': False,\n",
       " 'V_oo': False,\n",
       " 'V_cause': False,\n",
       " 'V_try': False,\n",
       " 'V_few': False,\n",
       " 'V_php': False,\n",
       " 'V_complete': False,\n",
       " 'V_health': False,\n",
       " 'V_regards': False,\n",
       " 'V_registered': False,\n",
       " 'V_cotten': False,\n",
       " 'V_form': False,\n",
       " 'V_china': False,\n",
       " 'V_david': False,\n",
       " 'V_private': False,\n",
       " 'V_bgcolor': False,\n",
       " 'V_update': False,\n",
       " 'V_required': False,\n",
       " 'V_too': False,\n",
       " 'V_third': False,\n",
       " 'V_years': False,\n",
       " 'V_between': False,\n",
       " 'V_mg': False,\n",
       " 'V_im': False,\n",
       " 'V_correct': False,\n",
       " 'V_drugs': False,\n",
       " 'V_duke': False,\n",
       " 'V_city': False,\n",
       " 'V_another': False,\n",
       " 'V_entex': True,\n",
       " 'V_him': False,\n",
       " 'V_valid': False,\n",
       " 'V_getting': False,\n",
       " 'V_man': False,\n",
       " 'V_systems': False,\n",
       " 'V_ever': False,\n",
       " 'V_original': False,\n",
       " 'V_mobile': False,\n",
       " 'V_commercial': False,\n",
       " 'V_2005': False,\n",
       " 'V_rita': False,\n",
       " 'V_found': False,\n",
       " 'V_however': False,\n",
       " 'V_east': False,\n",
       " 'V_morning': False,\n",
       " 'V_mark': False,\n",
       " 'V_expectations': False,\n",
       " 'V_regarding': False,\n",
       " 'V_field': False,\n",
       " 'V_pain': False,\n",
       " 'V_states': False,\n",
       " 'V_watch': False,\n",
       " 'V_stella': False,\n",
       " 'V_midcon': False,\n",
       " 'V_might': False,\n",
       " 'V_55': False,\n",
       " 'V_old': False,\n",
       " 'V_survey': False,\n",
       " 'V_weissman': False,\n",
       " 'V_either': False,\n",
       " 'V_text': False,\n",
       " 'V_suite': False,\n",
       " 'V_love': False,\n",
       " 'V_john': False,\n",
       " 'V_material': False,\n",
       " 'V_united': False,\n",
       " 'V_department': False,\n",
       " 'V_duty': False,\n",
       " 'V_removed': False,\n",
       " 'V_supply': False,\n",
       " 'V_equistar': False,\n",
       " 'V_zone': False,\n",
       " 'V_canada': False,\n",
       " 'V_paid': False,\n",
       " 'V_200': False,\n",
       " 'V_growth': False,\n",
       " 'V_operating': False,\n",
       " 'V_cs': False,\n",
       " 'V_bank': False,\n",
       " 'V_gif': False,\n",
       " 'V_ken': False,\n",
       " 'V_page': False,\n",
       " 'V_thu': False,\n",
       " 'V_exchange': False,\n",
       " 'V_risks': False,\n",
       " 'V_direct': False,\n",
       " 'V_wellhead': False,\n",
       " 'V_create': False,\n",
       " 'V_biz': False,\n",
       " 'V_james': False,\n",
       " 'V_already': False,\n",
       " 'V_70': False,\n",
       " 'V_potential': False,\n",
       " 'V_48': False,\n",
       " 'V_everyone': False,\n",
       " 'V_stacey': False,\n",
       " 'V_second': False,\n",
       " 'V_terms': False,\n",
       " 'V_server': False,\n",
       " 'V_area': False,\n",
       " 'V_costs': False,\n",
       " 'V_major': False,\n",
       " 'V_download': False,\n",
       " 'V_retail': False,\n",
       " 'V_wynne': False,\n",
       " 'V_really': False,\n",
       " 'V_credit': False,\n",
       " 'V_tabs': False,\n",
       " 'V_meyers': False,\n",
       " 'V_transfer': False,\n",
       " 'V_country': False,\n",
       " 'V_yourself': False,\n",
       " 'V_around': False,\n",
       " 'V_loss': False,\n",
       " 'V_spam': False,\n",
       " 'V_browser': False,\n",
       " 'V_anything': False,\n",
       " 'V_kind': False,\n",
       " 'V_add': False,\n",
       " 'V_flowed': False,\n",
       " 'V_actual': False,\n",
       " 'V_small': False,\n",
       " 'V_action': False,\n",
       " 'V_public': False,\n",
       " 'V_book': False,\n",
       " 'V_industrial': False,\n",
       " 'V_clear': False,\n",
       " 'V_fuels': False,\n",
       " 'V_remember': False,\n",
       " 'V_source': False,\n",
       " 'V_weight': False,\n",
       " 'V_thomas': False,\n",
       " 'V_ali': False,\n",
       " 'V_compliance': False,\n",
       " 'V_side': False,\n",
       " 'V_publisher': False,\n",
       " 'V_plan': False,\n",
       " 'V_unsubscribe': False,\n",
       " 'V_differ': False,\n",
       " 'V_period': False,\n",
       " 'V_open': False,\n",
       " 'V_weekend': False,\n",
       " 'V_charlotte': False,\n",
       " 'V_gathering': False,\n",
       " 'V_september': False,\n",
       " 'V_yahoo': False,\n",
       " 'V_aol': False,\n",
       " 'V_table': False,\n",
       " 'V_worldwide': False,\n",
       " 'V_version': False,\n",
       " 'V_reports': False,\n",
       " 'V_300': False,\n",
       " 'V_communications': False,\n",
       " 'V_ranch': False,\n",
       " 'V_meet': False,\n",
       " 'V_75': False,\n",
       " 'V_shut': False,\n",
       " 'V_yet': False,\n",
       " 'V_experience': False,\n",
       " 'V_}': False,\n",
       " 'V_56': False,\n",
       " 'V_tuesday': False,\n",
       " 'V_view': False,\n",
       " 'V_case': False,\n",
       " 'V_application': False,\n",
       " 'V_charge': False,\n",
       " 'V_papayoti': False,\n",
       " 'V_drive': False,\n",
       " 'V_press': False,\n",
       " 'V_weeks': False,\n",
       " 'V_something': False,\n",
       " 'V_target': False,\n",
       " 'V_computer': False,\n",
       " 'V_wanted': False,\n",
       " 'V_jennifer': False,\n",
       " 'V_jpg': False,\n",
       " 'V_mr': False,\n",
       " 'V_971': False,\n",
       " 'V_aware': False,\n",
       " 'V_technologies': False,\n",
       " 'V_portfolio': False,\n",
       " 'V_left': False,\n",
       " 'V_lannou': False,\n",
       " 'V_longer': False,\n",
       " 'V_cheap': False,\n",
       " 'V_34': False,\n",
       " 'V_mx': False,\n",
       " 'V_capital': False,\n",
       " 'V_receipt': False,\n",
       " 'V_body': False,\n",
       " 'V_124': False,\n",
       " 'V_paste': False,\n",
       " 'V_late': False,\n",
       " 'V_men': False,\n",
       " 'V_photoshop': False,\n",
       " 'V_devon': False,\n",
       " 'V_52': False,\n",
       " 'V_hard': False,\n",
       " 'V_carthage': False,\n",
       " 'V_img': False,\n",
       " 'V_options': False,\n",
       " 'V_\\\\': False,\n",
       " 'V_large': False,\n",
       " 'V_everything': False,\n",
       " 'V_uncertainties': False,\n",
       " 'V_play': False,\n",
       " 'V_scheduled': False,\n",
       " 'V_dec': False,\n",
       " 'V_thru': False,\n",
       " 'V_thought': False,\n",
       " 'V_feedback': False,\n",
       " 'V_valign': False,\n",
       " 'V_cotton': False,\n",
       " 'V_reason': False,\n",
       " 'V_expected': False,\n",
       " 'V_firm': False,\n",
       " 'V_network': False,\n",
       " 'V_problems': False,\n",
       " 'V_lose': False,\n",
       " 'V_brand': False,\n",
       " 'V_super': False,\n",
       " 'V_withers': False,\n",
       " 'V_fill': False,\n",
       " 'V_carlos': False,\n",
       " 'V_latest': False,\n",
       " 'V_considered': False,\n",
       " 'V_say': False,\n",
       " 'V_south': False,\n",
       " 'V_sources': False,\n",
       " 'V_follows': False,\n",
       " 'V_close': False,\n",
       " 'V_sex': False,\n",
       " 'V_middle': False,\n",
       " 'V_exception': False,\n",
       " 'V_valley': False,\n",
       " 'V_methanol': False,\n",
       " 'V_federal': False,\n",
       " 'V_bill': False,\n",
       " 'V_meaning': False,\n",
       " 'V_quick': False,\n",
       " 'V_voice': False,\n",
       " 'V_1266': False,\n",
       " 'V_letter': False,\n",
       " 'V_gcs': False,\n",
       " 'V_cec': False,\n",
       " 'V_33': False,\n",
       " 'V_box': False,\n",
       " 'V_hope': False,\n",
       " 'V_requirements': False,\n",
       " 'V_thing': False,\n",
       " 'V_certain': False,\n",
       " 'V_investors': False,\n",
       " 'V_logistics': False,\n",
       " 'V_{': False,\n",
       " 'V_employees': False,\n",
       " 'V_000000': False,\n",
       " 'V_begin': False,\n",
       " 'V_nothing': False,\n",
       " 'V_key': False,\n",
       " 'V_related': False,\n",
       " 'V_important': False,\n",
       " 'V_ensure': False,\n",
       " 'V_legal': False,\n",
       " 'V_ability': False,\n",
       " 'V_donald': False,\n",
       " 'V_nominations': False,\n",
       " 'V_4179': False,\n",
       " 'V_div': False,\n",
       " 'V_called': False,\n",
       " 'V_generic': False,\n",
       " 'V_ic': False,\n",
       " 'V_tell': False,\n",
       " 'V_availability': False,\n",
       " 'V_title': False,\n",
       " 'V_39': False,\n",
       " 'V_head': False,\n",
       " 'V_featured': False,\n",
       " 'V_publication': False,\n",
       " 'V_website': False,\n",
       " 'V_effort': False,\n",
       " 'V_~': False,\n",
       " 'V_mid': False,\n",
       " 'V_thursday': False,\n",
       " 'V_calls': False,\n",
       " 'V_updated': False,\n",
       " 'V_beaumont': False,\n",
       " 'V_features': False,\n",
       " 'V_moopid': False,\n",
       " 'V_hotlist': False,\n",
       " 'V_approximately': True,\n",
       " 'V_solution': False,\n",
       " 'V_rc': False,\n",
       " 'V_person': False,\n",
       " 'V_htm': False,\n",
       " 'V_hello': False,\n",
       " 'V_pricing': False,\n",
       " 'V_47': False,\n",
       " 'V_revision': False,\n",
       " 'V_times': False,\n",
       " 'V_requested': False,\n",
       " 'V_interested': False,\n",
       " 'V_different': False,\n",
       " 'V_confirm': False,\n",
       " 'V_usa': False,\n",
       " 'V_property': False,\n",
       " 'V_four': False,\n",
       " 'V_things': False,\n",
       " 'V_involve': False,\n",
       " 'V_subscribers': False,\n",
       " 'V_bryan': False,\n",
       " 'V_rd': False,\n",
       " 'V_safe': False,\n",
       " 'V_pharmacy': False,\n",
       " 'V_path': False,\n",
       " 'V_37': False,\n",
       " 'V_pc': False,\n",
       " 'V_etc': False,\n",
       " 'V_dear': False,\n",
       " 'V_fast': False,\n",
       " 'V_dave': False,\n",
       " 'V_listing': False,\n",
       " 'V_epson': False,\n",
       " 'V_secure': False,\n",
       " 'V_plans': False,\n",
       " 'V_projections': False,\n",
       " 'V_words': False,\n",
       " 'V_value': False,\n",
       " 'V_49': False,\n",
       " 'V_lauri': False,\n",
       " 'V_fund': False,\n",
       " 'V_sherlyn': False,\n",
       " 'V_offering': False,\n",
       " 'V_swing': False,\n",
       " 'V_producer': False,\n",
       " 'V_president': False,\n",
       " ...}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_featuresets[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9585185185185185"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a classifier and report accuracy\n",
    "train_set, test_set = bigram_featuresets[2700:], bigram_featuresets[:2700]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Bigram Features with Non-Alpha Characters Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to create feature sets for all sentences\n",
    "bigram_featuresets = [(bigram_document_features(d, filtered_word_features, bigram_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9540740740740741"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a classifier and report accuracy\n",
    "train_set, test_set = bigram_featuresets[2700:], bigram_featuresets[:2700]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Bigram Features with Stop Words Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to create feature sets for all sentences\n",
    "bigram_featuresets = [(bigram_document_features(d, no_stop_word_features, bigram_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9596296296296296"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a classifier and report accuracy\n",
    "train_set, test_set = bigram_featuresets[2700:], bigram_featuresets[:2700]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Bigram Features with Negation Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to create feature sets for all sentences\n",
    "bigram_featuresets = [(bigram_document_features(d, negation_word_features, bigram_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 600\n",
      "0 0.9683333333333334\n",
      "1 0.96\n",
      "2 0.9666666666666667\n",
      "3 0.9666666666666667\n",
      "4 0.9733333333333334\n",
      "mean accuracy 0.967\n"
     ]
    }
   ],
   "source": [
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "cross_validation_accuracy(num_folds, bigram_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. POS Tag Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004\n"
     ]
    }
   ],
   "source": [
    "# define feature sets using this function\n",
    "POS_featuresets = [(POS_features(d, word_features), c) for (d, c) in emaildocs]\n",
    "# number of features for document 0\n",
    "print(len(POS_featuresets[0][0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Subject', ':', 'entex', 'for', 'jan', '.', '2000', 'this', 'spreadsheet', 'shows', 'the', 'current', 'estimates', 'are', 'approximately', '12', ',', '000', '/', 'day', 'short', 'of', 'what', 'the', 'estimates', 'would', 'be', 'with', 'the', 'revised', 'entex', 'factor', '.', 'call', 'me', 'if', 'you', 'have', 'any', 'questions', '.', 'i', 'will', 'work', 'to', 'get', 'the', 'revised', 'estimates', 'into', 'pops', 'today', '.', 'gary'], 'ham')\n",
      "num nouns 14\n",
      "num verbs 10\n",
      "num adjectives 3\n",
      "num adverbs 1\n"
     ]
    }
   ],
   "source": [
    "# the first sentence\n",
    "print(emaildocs[0])\n",
    "# the pos tag features for this sentence\n",
    "print('num nouns', POS_featuresets[0][0]['nouns'])\n",
    "print('num verbs', POS_featuresets[0][0]['verbs'])\n",
    "print('num adjectives', POS_featuresets[0][0]['adjectives'])\n",
    "print('num adverbs', POS_featuresets[0][0]['adverbs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.962"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test the classifier\n",
    "train_set, test_set = POS_featuresets[1500:], POS_featuresets[:1500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 600\n",
      "0 0.97\n",
      "1 0.9616666666666667\n",
      "2 0.965\n",
      "3 0.9666666666666667\n",
      "4 0.9716666666666667\n",
      "mean accuracy 0.967\n"
     ]
    }
   ],
   "source": [
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "cross_validation_accuracy(num_folds, POS_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. POS Tag Features with Non-Alpha Characters Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature sets using this POS function and filtered word features\n",
    "POS_featuresets = [(POS_features(d, filtered_word_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 600\n",
      "0 0.9533333333333334\n",
      "1 0.9566666666666667\n",
      "2 0.965\n",
      "3 0.9666666666666667\n",
      "4 0.9716666666666667\n",
      "mean accuracy 0.9626666666666667\n"
     ]
    }
   ],
   "source": [
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "cross_validation_accuracy(num_folds, POS_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. POS Tag Features with Stop Words Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature sets using this POS function and no stop words features\n",
    "POS_featuresets = [(POS_features(d, no_stop_word_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 600\n",
      "0 0.9683333333333334\n",
      "1 0.9683333333333334\n",
      "2 0.9716666666666667\n",
      "3 0.9716666666666667\n",
      "4 0.975\n",
      "mean accuracy 0.9709999999999999\n"
     ]
    }
   ],
   "source": [
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "cross_validation_accuracy(num_folds, POS_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. POS Tag Features with Negation Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature sets using this POS function and no stop words features\n",
    "POS_featuresets = [(POS_features(d, negation_word_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 600\n",
      "0 0.9683333333333334\n",
      "1 0.9683333333333334\n",
      "2 0.9716666666666667\n",
      "3 0.9716666666666667\n",
      "4 0.975\n",
      "mean accuracy 0.9709999999999999\n"
     ]
    }
   ],
   "source": [
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "cross_validation_accuracy(num_folds, POS_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Subjectivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/brcro/OneDrive/Documents/Syracuse/IST 664 - Natural Language Processing/Week 8 - Sentiment Analysis/Lab/subjclueslen1-HLTEMNLP05.tff'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-225-5fe621e0ec09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:/Users/brcro/OneDrive/Documents/Syracuse/IST 664 - Natural Language Processing/Week 8 - Sentiment Analysis/Lab/subjclueslen1-HLTEMNLP05.tff\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mSL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadSubjectivity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# how many words are in the dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-171-343dfe1c26a0>\u001b[0m in \u001b[0;36mreadSubjectivity\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#     the four items of subjectivity information described above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreadSubjectivity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mflexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# initialize an empty dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msldict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/brcro/OneDrive/Documents/Syracuse/IST 664 - Natural Language Processing/Week 8 - Sentiment Analysis/Lab/subjclueslen1-HLTEMNLP05.tff'"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/brcro/OneDrive/Documents/Syracuse/IST 664 - Natural Language Processing/Week 8 - Sentiment Analysis/Lab/subjclueslen1-HLTEMNLP05.tff\"\n",
    "SL = readSubjectivity(path)\n",
    "# how many words are in the dictionary\n",
    "len(SL.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_featuresets = [(SL_features(d, word_features, SL), c) for (d, c) in emaildocs]\n",
    "len(SL_featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives the label of document 0\n",
    "print(SL_featuresets[0][1])\n",
    "# number of features for document 0\n",
    "len(SL_featuresets[0][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_featuresets[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the classifier using these features\n",
    "train_set, test_set = SL_featuresets[2700:], SL_featuresets[:2700]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Sentiment with Non-Alpha Characters Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_features has non-alpha characters removed from code above\n",
    "SL_featuresets = [(SL_features(d, word_features, SL), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the classifier using these features\n",
    "train_set, test_set = SL_featuresets[2700:], SL_featuresets[:2700]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 10\n",
    "cross_validation_accuracy(num_folds, SL_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Sci-Kit Learn with Unigram Featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use document_features function to create unigram feature sets for all sentences\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the feature sets to the csv file\n",
    "outpath = 'C:\\\\Users\\\\Brandon Croarkin\\\\Documents\\\\Education\\\\Syracuse\\\\NLP\\\\Final Project\\\\FinalProjectData\\\\ExternalClassifier\\SK_Features.csv'\n",
    "writeFeatureSets(featuresets, outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V_-</th>\n",
       "      <th>V_.</th>\n",
       "      <th>V_/</th>\n",
       "      <th>V_CM</th>\n",
       "      <th>V_:</th>\n",
       "      <th>V_the</th>\n",
       "      <th>V_to</th>\n",
       "      <th>V_ect</th>\n",
       "      <th>V_and</th>\n",
       "      <th>V_of</th>\n",
       "      <th>...</th>\n",
       "      <th>V_teams</th>\n",
       "      <th>V_mean</th>\n",
       "      <th>V_memo</th>\n",
       "      <th>V_liquids</th>\n",
       "      <th>V_situation</th>\n",
       "      <th>V_und</th>\n",
       "      <th>V_commission</th>\n",
       "      <th>V_wiil</th>\n",
       "      <th>V_deposit</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     V_-   V_.    V_/  V_CM   V_:  V_the   V_to  V_ect  V_and   V_of  ...    \\\n",
       "0  False  True   True  True  True   True   True  False  False   True  ...     \n",
       "1   True  True   True  True  True   True   True  False   True  False  ...     \n",
       "2   True  True   True  True  True   True   True  False   True   True  ...     \n",
       "3   True  True  False  True  True   True  False  False   True  False  ...     \n",
       "4   True  True  False  True  True   True   True  False   True   True  ...     \n",
       "\n",
       "   V_teams  V_mean  V_memo  V_liquids  V_situation  V_und  V_commission  \\\n",
       "0    False   False   False      False        False  False         False   \n",
       "1    False   False   False      False        False  False         False   \n",
       "2    False   False   False      False        False  False         False   \n",
       "3    False   False   False      False        False  False         False   \n",
       "4    False   False   False      False        False  False         False   \n",
       "\n",
       "   V_wiil  V_deposit  class  \n",
       "0   False      False    ham  \n",
       "1   False      False    ham  \n",
       "2   False      False    ham  \n",
       "3   False      False   spam  \n",
       "4   False      False    ham  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the csv file to use with sk-learn\n",
    "sk_featureset = pd.read_csv(outpath, encoding=\"latin-1\")\n",
    "sk_featureset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split out the data and the labels\n",
    "X = pd.DataFrame(sk_featureset.iloc[:,:-1])\n",
    "y = pd.factorize(sk_featureset.loc[:,'class'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9766666666666667\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V_2000         0.044301\n",
       "V_hpl          0.025915\n",
       "V_gas          0.022925\n",
       "V_enron        0.022448\n",
       "V_thanks       0.021131\n",
       "V_daren        0.019841\n",
       "V_!            0.019145\n",
       "V_http         0.018684\n",
       "V_ect          0.017716\n",
       "V_forwarded    0.017356\n",
       "V_meter        0.015525\n",
       "V_attached     0.015401\n",
       "V_pm           0.014163\n",
       "V_hou          0.013613\n",
       "V_here         0.011541\n",
       "V_cc           0.011482\n",
       "V_%            0.010194\n",
       "V_please       0.009698\n",
       "V_deal         0.009459\n",
       "V_&            0.009437\n",
       "dtype: float64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\n",
    "feature_imp[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x183e6fdbf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp[:20], y=feature_imp[:20].index)\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Term Frequency Inverse Data Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', 0.0702740884113106),\n",
       " ('.', 0.04647767469799106),\n",
       " ('/', 0.03455722040534828),\n",
       " (',', 0.0318569378629113),\n",
       " (':', 0.022492157778815993),\n",
       " ('the', 0.02085836169881421),\n",
       " ('to', 0.016091848539455826),\n",
       " ('ect', 0.011857883378940576),\n",
       " ('and', 0.011355925604574072),\n",
       " ('of', 0.009570568866938085),\n",
       " ('@', 0.00898518320763532),\n",
       " ('a', 0.008828060691005361),\n",
       " ('for', 0.008113361810051392),\n",
       " ('?', 0.007316625508910098),\n",
       " ('you', 0.007146988809539701),\n",
       " ('in', 0.00712335090880776),\n",
       " ('this', 0.006212596498253576),\n",
       " ('is', 0.006201472780262075),\n",
       " ('hou', 0.00616810162628757),\n",
       " ('on', 0.005621648979955061)]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create frequency distribution of the top 50 words from both data sets.\n",
    "# Since the word frequency items are a pair of (word, frequency), \n",
    "# we can use item[0] to get the word and item[1] to get the frequency.  \n",
    "# Printing the string \\t inserts a tab into the output, \n",
    "# so that the frequency numbers line up.\n",
    "\n",
    "## Using Spam\n",
    "fdist = nltk.FreqDist(all_words_list)\n",
    "fdistkeys = list(fdist.keys())\n",
    "fdistkeys[:2000]\n",
    "\n",
    "topkeys = fdist.most_common(2000)\n",
    "#for pair in topkeys:\n",
    "    #print(pair)\n",
    "\n",
    "# Normalize\n",
    "numwords = len(all_words_list)\n",
    "topkeysnormalized = [(word, freq / numwords) for (word, freq) in topkeys]\n",
    "topkeysnormalized[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use document_features function to create unigram feature sets for all sentences\n",
    "TFIDF_featuresets = [(TFIDF_document_features(d, word_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TFIDF_featuresets[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'V_-': 0, 'V_.': 0.04647767469799106, 'V_/': 0.03455722040534828, 'V_,': 0.0318569378629113, 'V_:': 0.022492157778815993, 'V_the': 0.02085836169881421, 'V_to': 0.016091848539455826, 'V_ect': 0, 'V_and': 0, 'V_of': 0.009570568866938085, 'V_@': 0, 'V_a': 0, 'V_for': 0.008113361810051392, 'V_?': 0, 'V_you': 0.007146988809539701, 'V_in': 0, 'V_this': 0.006212596498253576, 'V_is': 0, 'V_hou': 0, 'V_on': 0, 'V_i': 0.004681694809673185, \"V_'\": 0, 'V_)': 0, 'V_=': 0, 'V_(': 0, 'V_enron': 0, 'V_Subject': 0.0041713942468130545, 'V_!': 0, 'V_be': 0.004014271730183096, 'V_your': 0, 'V_2000': 0.003929453380497897, 'V_that': 0, 'V_with': 0.003481723731339963, 'V_from': 0, 'V__': 0, 'V_will': 0.003111860108122539, 'V_have': 0.003071536630403346, 'V_we': 0, 'V_s': 0, 'V_as': 0, 'V_are': 0.002881042959798883, 'V_it': 0, 'V_$': 0, 'V_>': 0, 'V_or': 0, 'V_3': 0, 'V_at': 0, 'V_not': 0, 'V_by': 0, 'V_please': 0, 'V_``': 0, 'V_com': 0, 'V_if': 0.0023498854257046876, 'V_|': 0, 'V_1': 0, 'V_;': 0, 'V_#': 0, 'V_our': 0, 'V_me': 0.00199392644997664, 'V_2': 0, 'V_e': 0, 'V_subject': 0, 'V_gas': 0, 'V_all': 0, 'V_00': 0, 'V_%': 0, 'V_*': 0, 'V_meter': 0, 'V_am': 0, 'V_can': 0, 'V_any': 0.0016435293332443435, 'V_cc': 0, 'V_d': 0, 'V_pm': 0, 'V_000': 0.00156149191305702, 'V_deal': 0, 'V_corp': 0, 'V_http': 0, 'V_has': 0, 'V_no': 0, 'V_an': 0, 'V_0': 0, 'V_re': 0, 'V_4': 0, 'V_10': 0, 'V_new': 0, 'V_hpl': 0, 'V_company': 0, 'V_5': 0, 'V_was': 0, 'V_thanks': 0, 'V_up': 0, 'V_7': 0, 'V_get': 0.0011443524883757146, 'V_t': 0, 'V_99': 0, 'V_&': 0, 'V_know': 0, 'V_information': 0, 'V_may': 0}\n",
      "ham\n"
     ]
    }
   ],
   "source": [
    "print(TFIDF_featuresets[0][0])\n",
    "print(TFIDF_featuresets[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9596296296296296"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the classifier using these features\n",
    "train_set, test_set = TFIDF_featuresets[2700:], TFIDF_featuresets[:2700]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
